{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Z4I91-hbsv"
   },
   "source": [
    "# **Marker Position prediction from IMU data using deep learning.**\n",
    "\n",
    "As a part of the Article: \n",
    "DOI: (Will be added)\n",
    "\n",
    "# Author: Vaibhav R. Shah and Philippe C. Dixon\n",
    "\n",
    "We recomand you run on jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98IaL4T6h0IK"
   },
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4804,
     "status": "ok",
     "timestamp": 1735374365104,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "9dLzqEH9hs96"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GaitLab2Go as GL2G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9eJQjo6h35n"
   },
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1735374523989,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "LXlNzEeAh_gu"
   },
   "outputs": [],
   "source": [
    "lab=GL2G.data_processing()\n",
    "fld='./data/pp054/imu'\n",
    "ext='.zoo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tUIqPFM_2nD"
   },
   "source": [
    "Converting .zoo files to .pkl files (Compatible with GaitLab2Go library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1735374525764,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "4C6mo5F7ASjN",
    "outputId": "0d162c1b-7584-4682-8b41-f3194a044f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ./data/pp054/imu/pp054_WT_01_cycle_1.zoo\n",
      "Extracting complete ./data/pp054/imu/pp054_WT_01_cycle_1.zoo\n",
      "saving as pickle file to ./data/pp054/imu/pp054_WT_01_cycle_1.pkl\n",
      "Extracting data from ./data/pp054/imu/pp054_WT_01_cycle_2.zoo\n",
      "Extracting complete ./data/pp054/imu/pp054_WT_01_cycle_2.zoo\n",
      "saving as pickle file to ./data/pp054/imu/pp054_WT_01_cycle_2.pkl\n",
      "['pp054']\n",
      "['./data/pp054/imu/RTHI.pkl' './data/pp054/imu/RASI.pkl'\n",
      " './data/pp054/imu/LTHI.pkl' './data/pp054/imu/LASI.pkl'\n",
      " './data/pp054/imu/RTOE.pkl' './data/pp054/imu/RKNE.pkl'\n",
      " './data/pp054/imu/LTIB.pkl'\n",
      " './data/pp054/imu/pp054marker_prediction_data.pkl'\n",
      " './data/pp054/imu/pp054_WT_01_cycle_2.pkl' './data/pp054/imu/LPSI.pkl'\n",
      " './data/pp054/imu/pp054_WT_01_cycle_1.pkl' './data/pp054/imu/RANK.pkl'\n",
      " './data/pp054/imu/RHEE.pkl' './data/pp054/imu/LKNE.pkl'\n",
      " './data/pp054/imu/LTOE.pkl' './data/pp054/imu/LANK.pkl'\n",
      " './data/pp054/imu/LHEE.pkl' './data/pp054/imu/RTIB.pkl'\n",
      " './data/pp054/imu/RPSI.pkl']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'Ncycle_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(subject_list)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_list)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mlab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_subject_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load the first file in the file list (pickled data) into a Pandas DataFrame 'x'\u001b[39;00m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(fl[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Code/Shah-Marker-position-prediction/GaitLab2Go.py:178\u001b[0m, in \u001b[0;36mdata_processing.process_subject_files\u001b[0;34m(self, subject_list, file_list)\u001b[0m\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(fn)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Apply the ncycle_data function to the data\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncycle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Print a message indicating the file is being saved\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaving file to\u001b[39m\u001b[38;5;124m'\u001b[39m, fn)\n",
      "File \u001b[0;32m~/Code/Shah-Marker-position-prediction/GaitLab2Go.py:153\u001b[0m, in \u001b[0;36mdata_processing.ncycle_data\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mncycle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create an empty dictionary Ncycle_data within the object x.\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     x\u001b[38;5;241m.\u001b[39mNcycle_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Iterate through each variable in the keys of the data attribute of object x.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;66;03m# Create an empty dictionary for each variable within the Ncycle_data dictionary.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'Ncycle_data'"
     ]
    }
   ],
   "source": [
    "# Get the IMU variables from lab's variables zoo and store in 'variables'\n",
    "variables = lab.variables_zoo_IMU()\n",
    "\n",
    "# Remove the last 18 variables from the list\n",
    "#variables = variables[:-18]\n",
    "\n",
    "# Find files in the specified folder 'fld' with the extension specified in 'ext'\n",
    "fl = lab.find_files(path=fld, ext=ext)\n",
    "\n",
    "# Convert files from Zoo format to dictionary format using 'variables' and store the result\n",
    "lab.zoo2dictionary(fl, variables)\n",
    "\n",
    "# Set file extension to '.pkl' for further processing\n",
    "ext = '.pkl'\n",
    "\n",
    "# Find files again in 'fld' directory, but now with the new extension '.pkl'\n",
    "fl = lab.find_files(path=fld, ext=ext)\n",
    "\n",
    "# Specify the list of subjects for processing (in this case, a single subject 'pp054')\n",
    "subject_list = ['pp054']\n",
    "\n",
    "# List of files to process (found previously with the '.pkl' extension)\n",
    "file_list = fl\n",
    "\n",
    "# Process the specified subject files using the subject and file lists\n",
    "print(subject_list)\n",
    "print(file_list)\n",
    "lab.process_subject_files(subject_list, file_list)\n",
    "\n",
    "# Load the first file in the file list (pickled data) into a Pandas DataFrame 'x'\n",
    "x = pd.read_pickle(fl[0])\n",
    "\n",
    "# Extract a list of variable names from the 'data' attribute in the loaded file\n",
    "variable = list(x.data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1735374544944,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "E9MlTmr4iVrf"
   },
   "outputs": [],
   "source": [
    "# Function to process data for specific subjects and tasks, based on provided file list and variables\n",
    "def process_subject_data_task(subject_list, file_list, variable):\n",
    "    # Initialize an empty dictionary 'data' to store processed data for each variable\n",
    "    data = {}\n",
    "\n",
    "    # Initialize each variable in 'data' with an array of zeros with shape (1, 101)\n",
    "    for var in variable:\n",
    "        data[var] = np.zeros([1, 101])\n",
    "\n",
    "    # Add subject and task keys to 'data' with initial values\n",
    "    data['subject'] = np.array('test')\n",
    "    data['task'] = np.array([99])\n",
    "\n",
    "    # Loop over each subject in the subject list\n",
    "    for sub in subject_list:\n",
    "        # Loop over each file in file_list that contains the subject identifier\n",
    "        for fn in file_list[np.char.find(file_list, sub) > 0]:\n",
    "            print(fn)  # Print file name for debugging\n",
    "\n",
    "            # Extract task identifier from file name\n",
    "            Task = fn.split('/')[-1].split('_')[2]\n",
    "\n",
    "            # Determine the task type based on Task identifier and assign task_num accordingly\n",
    "            if Task == '01' or Task == '05':\n",
    "                task_num = 0\n",
    "                print('walk')\n",
    "            elif Task == '02' or Task == '04':\n",
    "                task_num = 1\n",
    "                # Uncomment below to print 'jog' if needed\n",
    "                # print('jog')\n",
    "            elif Task == '03':\n",
    "                task_num = 2\n",
    "                # Uncomment below to print 'run' if needed\n",
    "                # print('run')\n",
    "\n",
    "            # Load data from the file as a Pandas DataFrame 'x'\n",
    "            x = pd.read_pickle(fn)\n",
    "\n",
    "            # Append the subject identifier to 'data' under 'subject' key\n",
    "            data['subject'] = np.append(data['subject'], sub)\n",
    "\n",
    "            # Append the task number to 'data' under 'task' key\n",
    "            data['task'] = np.append(data['task'], task_num)\n",
    "\n",
    "            # Loop over each variable in the loaded data\n",
    "            for var in x.data.keys():\n",
    "                # Append the Ncycle data for each variable along axis 0\n",
    "                data[var] = np.append(data[var], x.Ncycle_data[var], axis=0)\n",
    "\n",
    "    # Return the processed data dictionary\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1735374545869,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "3Z0fU_v9iZ9_",
    "outputId": "020cdad9-f961-4dc7-e129-bf9379ef250d"
   },
   "outputs": [],
   "source": [
    "# Assign the subject list to the variable 'subject'\n",
    "subject = subject_list\n",
    "\n",
    "# Assign the file list to 'files' (previously found using lab.find_files)\n",
    "files = fl  # Replace with your actual file list if needed\n",
    "\n",
    "# Call the process_subject_data_task function with the subject list, file list, and variable list\n",
    "result_data = process_subject_data_task(subject, files, variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1735374548313,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "Nfk_vE6bihZ3"
   },
   "outputs": [],
   "source": [
    "def process_knee_data(data,sensor,Marker):\n",
    "# Create an empty dictionary to store processed knee data\n",
    "    knee_data = {}\n",
    "\n",
    "    # Loop through keys in the input data\n",
    "    for var in data.keys():\n",
    "        # Include only specific variables related to shank, thigh, knee angles, and subject\n",
    "        if sensor[0] in var or sensor[1] in var or sensor[2] in var or Marker in var or 'subject' in var:\n",
    "            knee_data[var] = data[var]\n",
    "\n",
    "    # Get the shape of the 'shankR_Acc_X' array (assuming it exists)\n",
    "    x = knee_data[f'{sensor[0]}_Acc_X'].shape\n",
    "\n",
    "    # Lambda function to reshape and concatenate accelerometer and gyroscope data\n",
    "    reshape_and_concat = lambda acc_gyr: np.concatenate(\n",
    "        [knee_data[f'{part}_{axis}'].reshape(x[0], x[1], 1)\n",
    "          for part in sensor#,'footR','shankL','thighL','footL']\n",
    "          for axis in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Gyr_X', 'Gyr_Y', 'Gyr_Z']], axis=2)\n",
    "\n",
    "    # Reshape and concatenate accelerometer and gyroscope data\n",
    "    nA=map(reshape_and_concat,['Acc'])\n",
    "    nA1=list(nA)\n",
    "    nA1=nA1[0]\n",
    "\n",
    "    # Create a dictionary containing processed data\n",
    "    knee_d = {\n",
    "        'train': nA1,\n",
    "        'subject': knee_data['subject'],\n",
    "        #'test': nE,\n",
    "        'task':data['task']\n",
    "    }\n",
    "\n",
    "    return knee_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1735374551025,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "FBM_IiD91e5X"
   },
   "outputs": [],
   "source": [
    "# List of markers representing body landmarks\n",
    "markers = ['LASI', 'RASI', 'LPSI', 'RPSI', 'LTHI', 'LKNE', 'LTIB', 'LANK', 'LHEE', 'LTOE',\n",
    "           'RTHI', 'RKNE', 'RTIB', 'RANK', 'RHEE', 'RTOE']\n",
    "\n",
    "# Nested list of sensor groups, each containing sensors associated with specific body segments\n",
    "sensors = [['trunk', 'thighR', 'thighL'],    # Sensors for trunk and thighs\n",
    "           ['thighR', 'shankR', 'footR'],    # Sensors for right thigh, shank, and foot\n",
    "           ['thighL', 'shankL', 'footL']]    # Sensors for left thigh, shank, and foot\n",
    "\n",
    "# Mapping each marker to the appropriate sensor group\n",
    "# 0 corresponds to the first group, 1 to the second group, 2 to the third group\n",
    "nums = [0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Iterate over all markers\n",
    "for i in range(0, len(markers)):\n",
    "    # Call the process_knee_data function for each marker\n",
    "    # Pass the corresponding sensor group (from nums) and marker name\n",
    "    knee_d = process_knee_data(result_data, sensor=sensors[nums[i]], Marker=markers[i])\n",
    "\n",
    "    # Save the processed data as a pickle file\n",
    "    # The file is saved in a directory with the marker's name as the filename\n",
    "    pd.to_pickle(knee_d, f'./data/pp054/imu/{markers[i]}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBD7p-Z8i5aV"
   },
   "source": [
    "# Testing deep learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1735374559431,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "kR5psAAmAlv8"
   },
   "outputs": [],
   "source": [
    "# Path to the folder containing the marker position pickle files\n",
    "fld = './data/pp054/imu'\n",
    "\n",
    "# Load pickle file for the LASI marker (Left Anterior Superior Iliac Spine)\n",
    "data_LASI = pd.read_pickle(f'{fld}//LASI.pkl')\n",
    "\n",
    "# Load pickle file for the RASI marker (Right Anterior Superior Iliac Spine)\n",
    "data_RASI = pd.read_pickle(f'{fld}//RASI.pkl')\n",
    "\n",
    "# Load pickle file for the LPSI marker (Left Posterior Superior Iliac Spine)\n",
    "data_LPSI = pd.read_pickle(f'{fld}//LPSI.pkl')\n",
    "\n",
    "# Load pickle file for the RPSI marker (Right Posterior Superior Iliac Spine)\n",
    "data_RPSI = pd.read_pickle(f'{fld}//RPSI.pkl')\n",
    "\n",
    "# Load pickle file for the LTHI marker (Left Thigh)\n",
    "data_LTHI = pd.read_pickle(f'{fld}//LTHI.pkl')\n",
    "\n",
    "# Load pickle file for the LKNE marker (Left Knee)\n",
    "data_LKNE = pd.read_pickle(f'{fld}//LKNE.pkl')\n",
    "\n",
    "# Load pickle file for the LTIB marker (Left Tibia)\n",
    "data_LTIB = pd.read_pickle(f'{fld}//LTIB.pkl')\n",
    "\n",
    "# Load pickle file for the LANK marker (Left Ankle)\n",
    "data_LANK = pd.read_pickle(f'{fld}//LANK.pkl')\n",
    "\n",
    "# Load pickle file for the LHEE marker (Left Heel)\n",
    "data_LHEE = pd.read_pickle(f'{fld}//LHEE.pkl')\n",
    "\n",
    "# Load pickle file for the LTOE marker (Left Toe)\n",
    "data_LTOE = pd.read_pickle(f'{fld}//LTOE.pkl')\n",
    "\n",
    "# Load pickle file for the RTHI marker (Right Thigh)\n",
    "data_RTHI = pd.read_pickle(f'{fld}//RTHI.pkl')\n",
    "\n",
    "# Load pickle file for the RKNE marker (Right Knee)\n",
    "data_RKNE = pd.read_pickle(f'{fld}//RKNE.pkl')\n",
    "\n",
    "# Load pickle file for the RTIB marker (Right Tibia)\n",
    "data_RTIB = pd.read_pickle(f'{fld}//RTIB.pkl')\n",
    "\n",
    "# Load pickle file for the RANK marker (Right Ankle)\n",
    "data_RANK = pd.read_pickle(f'{fld}//RANK.pkl')\n",
    "\n",
    "# Load pickle file for the RHEE marker (Right Heel)\n",
    "data_RHEE = pd.read_pickle(f'{fld}//RHEE.pkl')\n",
    "\n",
    "# Load pickle file for the RTOE marker (Right Toe)\n",
    "data_RTOE = pd.read_pickle(f'{fld}//RTOE.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1735375391339,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "HNn405TSAqS_"
   },
   "outputs": [],
   "source": [
    "# Get the shape of the 'train' dataset from the LASI marker data\n",
    "shape_train = data_LASI['train'].shape\n",
    "\n",
    "# Reshape the 'train' dataset for LASI to add an extra dimension (e.g., channel dimension)\n",
    "data_LASI['train'] = data_LASI['train'].reshape(shape_train[0], shape_train[1], shape_train[2], 1)\n",
    "\n",
    "# Reshape the 'train' dataset for LKNE and RKNE similarly\n",
    "data_LKNE['train'] = data_LKNE['train'].reshape(shape_train[0], shape_train[1], shape_train[2], 1)\n",
    "data_RKNE['train'] = data_RKNE['train'].reshape(shape_train[0], shape_train[1], shape_train[2], 1)\n",
    "\n",
    "# Combine multiple sensor datasets along the last dimension (axis=3) to form a single training dataset\n",
    "# Includes subsets of LASI, LKNE, and RKNE data\n",
    "train = np.concatenate((\n",
    "    data_LASI['train'][:, :, 0:3, :], data_LASI['train'][:, :, 3:6, :], data_LASI['train'][:, :, 6:9, :],\n",
    "    data_LASI['train'][:, :, 9:12, :], data_LASI['train'][:, :, 12:15, :], data_LASI['train'][:, :, 15:18, :],\n",
    "    data_LKNE['train'][:, :, 6:9, :], data_LKNE['train'][:, :, 9:12, :], data_LKNE['train'][:, :, 12:15, :], data_LKNE['train'][:, :, 15:18, :],\n",
    "    data_RKNE['train'][:, :, 6:9, :], data_RKNE['train'][:, :, 9:12, :], data_RKNE['train'][:, :, 12:15, :], data_RKNE['train'][:, :, 15:18, :]\n",
    "), axis=3)\n",
    "\n",
    "\n",
    "# Iterate over specific indices of the training dataset\n",
    "# Convert gyroscope data (odd indices) from degrees to radians and scale accelerometer data (even indices)\n",
    "for i in [1, 3, 5, 7, 9, 11, 13]:\n",
    "    train[:, :, :, i] = np.deg2rad(train[:, :, :, i])  # Convert degrees to radians\n",
    "    train[:, :, :, i - 1] = train[:, :, :, i - 1] * 9.81  # Scale accelerometer data to m/s² (gravity factor)\n",
    "\n",
    "# Left shank sensor\n",
    "# Define a 180-degree rotation matrix around the z-axis for reorienting gyroscope and accelerometer data\n",
    "R_z_180 = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, -1, 0],\n",
    "    [0, 0, -1]\n",
    "])\n",
    "# Apply the rotation matrix to left shank gyroscope and accelerometer data\n",
    "train[:, :, :, 4] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 4])\n",
    "train[:, :, :, 5] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 5])\n",
    "\n",
    "# Left foot sensor\n",
    "# Define a 180-degree rotation matrix for reorienting data\n",
    "R_z_180 = np.array([\n",
    "    [-1, 0, 0],\n",
    "    [0, -1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "# Apply the rotation matrix to left foot gyroscope and accelerometer data\n",
    "train[:, :, :, 8] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 8])\n",
    "train[:, :, :, 9] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 9])\n",
    "\n",
    "# Right foot sensor\n",
    "# Define a 180-degree rotation matrix for reorienting data\n",
    "R_z_180 = np.array([\n",
    "    [-1, 0, 0],\n",
    "    [0, -1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "# Apply the rotation matrix to right foot gyroscope and accelerometer data\n",
    "train[:, :, :, 12] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 12])\n",
    "train[:, :, :, 13] = np.einsum('ij,klj->kli', R_z_180, train[:, :, :, 13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1735368491756,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "8uo5by8iA8k2"
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "def Normalized_gait( A):\n",
    "        # Generate an array 'x' with values from 0 to the number of columns in A\n",
    "        x = np.arange(A.shape[-1])\n",
    "\n",
    "        # Replace any NaN (Not a Number) values in array A with 0\n",
    "        A = np.where(np.isnan(A) == 1, 0, A)\n",
    "\n",
    "        # Interpolate values using cubic interpolation with 101 points\n",
    "        # Interpolate the data along axis 0 (assuming x is the axis)\n",
    "        Y = interpolate.interp1d(x, A, kind='cubic')(np.linspace(x.min(), x.max(), 101))\n",
    "\n",
    "        # Return the interpolated values\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1735374617040,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "cD84-ALzA9IK"
   },
   "outputs": [],
   "source": [
    "def train_test(data_LASI, train, Select_subject='S01'):\n",
    "    \"\"\"\n",
    "    Function to process and filter sensor data, with an option to select a specific subject.\n",
    "\n",
    "    Args:\n",
    "      data_LASI: A dictionary containing sensor data, typically structured with 'subject', 'task', and sensor values.\n",
    "      train: A numpy array or similar data structure containing training data.\n",
    "      Select_subject: A string specifying the subject to filter data for. Default is 'S01'.\n",
    "\n",
    "    Returns:\n",
    "      x_test_f: The filtered sensor data corresponding to the selected subject.\n",
    "      subject_test_f: The subject labels for the selected subject.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the training data and relevant fields from the data_LASI dictionary\n",
    "    X = train[:, :, :]  # Selecting all the dimensions from the training data array\n",
    "    Subject = data_LASI['subject'][:]  # Extract subject labels from the data_LASI dictionary\n",
    "    task = data_LASI['task'][:]  # Extract task information (currently unused in the function)\n",
    "\n",
    "    # Create a new array `nx` to store the Euclidean norm of the 3D accelerometer data (x, y, z)\n",
    "    nx = np.zeros((X.shape[0], X.shape[1], 14))  # Initialize an empty array for the norm calculation (14 for each data point)\n",
    "\n",
    "    # Loop over each time-point (or sample) and calculate the Euclidean norm of the (x, y, z) accelerometer data\n",
    "    for i in range(0, X.shape[3]):\n",
    "        # Calculating the Euclidean norm of the accelerometer data (sqrt(x^2 + y^2 + z^2))\n",
    "        nx[:, :, i] = np.sqrt(X[:, :, 0, i]**2 + X[:, :, 1, i]**2 + X[:, :, 2, i]**2)\n",
    "\n",
    "    # Create a new array `nx1` to concatenate the calculated norm with the original accelerometer data\n",
    "    nx1 = np.zeros((X.shape[0], X.shape[1], 4, X.shape[3]))  # New shape: adding 4th channel (norm) to the data\n",
    "\n",
    "    # Loop over each sample in the time-series and concatenate the norm along the 3rd axis (channel-wise)\n",
    "    for i in range(0, X.shape[3]):\n",
    "        nx1[:, :, :, i] = np.concatenate((X[:, :, :, i], nx[:, :, i:i + 1]), axis=2)\n",
    "\n",
    "    # Update the original data array `X` to include the new sensor data with norms\n",
    "    X = nx1\n",
    "\n",
    "    # Find the indices of the selected subject in the dataset\n",
    "    rows = np.where(Subject == Select_subject)[0]\n",
    "\n",
    "    # Filter the training data to only include the selected subject\n",
    "    x_test_f = X[rows]\n",
    "\n",
    "    # Extract the subject labels for the selected subject\n",
    "    subject_test_f = Subject[rows]\n",
    "\n",
    "    # Return the filtered data and subject labels for the selected subject\n",
    "    return x_test_f, subject_test_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1735374620056,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "AB5099ZTBGvQ"
   },
   "outputs": [],
   "source": [
    "# prompt: filter data with 4th order butterworth filter at 6hz cutoff and 100 hz sampaling rate\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff=6, fs=100, order=4):\n",
    "  \"\"\"\n",
    "  Applies a Butterworth low-pass filter to the input data.\n",
    "\n",
    "  Args:\n",
    "    data: The input data (e.g., a NumPy array).\n",
    "    cutoff: The cutoff frequency in Hz.\n",
    "    fs: The sampling frequency in Hz.\n",
    "    order: The order of the Butterworth filter.\n",
    "\n",
    "  Returns:\n",
    "    The filtered data.\n",
    "  \"\"\"\n",
    "  nyquist = 0.5 * fs\n",
    "  normal_cutoff = cutoff / nyquist\n",
    "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "  y = filtfilt(b, a, data)\n",
    "  return y\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'X' is your data (3D NumPy array) and you want to filter along the time axis (axis=1)\n",
    "# and have a sampling rate of 100 Hz\n",
    "fs = 200\n",
    "cutoff = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1735368495495,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "UE4ktTMEBHY8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def pred_restric_loss(y_true, y_pred):\n",
    "    \"\"\"Calculates a loss based on the differences between predictions and true values\n",
    "    at progressively larger temporal intervals. This ensures the model's predictions\n",
    "    maintain consistency over time.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth values with shape [batch, time, height, width].\n",
    "        y_pred (tensor): Predicted values with shape [batch, time, height, width].\n",
    "\n",
    "    Returns:\n",
    "        tensor: The restricted prediction loss.\n",
    "    \"\"\"\n",
    "    # Compute differences between adjacent time steps at various intervals for predictions\n",
    "    yp_1 = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:]  # 1-time-step difference\n",
    "    yp_2 = y_pred[:,2:,:,:] - y_pred[:,:-2,:,:]  # 2-time-step difference\n",
    "    yp_3 = y_pred[:,3:,:,:] - y_pred[:,:-3,:,:]  # 3-time-step difference\n",
    "    yp_4 = y_pred[:,4:,:,:] - y_pred[:,:-4,:,:]  # 4-time-step difference\n",
    "    yp_5 = y_pred[:,5:,:,:] - y_pred[:,:-5,:,:]  # 5-time-step difference\n",
    "\n",
    "    # Compute differences between adjacent time steps at various intervals for ground truth\n",
    "    yt_1 = y_true[:,1:,:,:] - y_true[:,:-1,:,:]\n",
    "    yt_2 = y_true[:,2:,:,:] - y_true[:,:-2,:,:]\n",
    "    yt_3 = y_true[:,3:,:,:] - y_true[:,:-3,:,:]\n",
    "    yt_4 = y_true[:,4:,:,:] - y_true[:,:-4,:,:]\n",
    "    yt_5 = y_true[:,5:,:,:] - y_true[:,:-5,:,:]\n",
    "\n",
    "    # Calculate mean squared error (MSE) for each temporal interval difference\n",
    "    ym1 = tf.reduce_mean(tf.square(yp_1 - yt_1))\n",
    "    ym2 = tf.reduce_mean(tf.square(yp_2 - yt_2))\n",
    "    ym3 = tf.reduce_mean(tf.square(yp_3 - yt_3))\n",
    "    ym4 = tf.reduce_mean(tf.square(yp_4 - yt_4))\n",
    "    ym5 = tf.reduce_mean(tf.square(yp_5 - yt_5))\n",
    "\n",
    "    # Aggregate MSEs for final loss value\n",
    "    mse1 = ym1 + ym2 + ym3 + ym4 + ym5\n",
    "\n",
    "    # Scale the loss by a factor of 2\n",
    "    mse = mse1 * 2\n",
    "    return mse\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Calculates the Mean Squared Error (MSE) loss.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth values.\n",
    "        y_pred (tensor): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        tensor: The MSE loss value.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def mse_loss_top_24(y_true, y_pred):\n",
    "    \"\"\"Calculates the MSE for the top 24 most significant error values.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth values.\n",
    "        y_pred (tensor): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        tensor: The averaged MSE of the top 24 largest errors.\n",
    "    \"\"\"\n",
    "    # Compute the mean squared error along the batch and temporal axes\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n",
    "    mse = tf.reduce_mean(mse, axis=0)\n",
    "\n",
    "    # Reshape and sort the MSE values in descending order\n",
    "    mse = tf.reshape(mse, [48])\n",
    "    mse = tf.sort(mse, direction='DESCENDING')\n",
    "\n",
    "    # Select the top 24 largest errors and compute their mean\n",
    "    mse = mse[:24]\n",
    "    return tf.reduce_mean(mse)\n",
    "\n",
    "@tf.function\n",
    "def custom_biomech_loss(y_true, y_pred):\n",
    "    \"\"\"Custom biomechanical loss function to account for both spatial and temporal constraints.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Ground truth values.\n",
    "        y_pred (tensor): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        tensor: The computed loss value.\n",
    "    \"\"\"\n",
    "    # Compute the overall MSE\n",
    "    mse = mse_loss(y_true, y_pred)\n",
    "\n",
    "    # Calculate separate MSEs for specific biomechanical features (e.g., right and left foot)\n",
    "    mse_R_foot = mse_loss(y_true[..., 7:9], y_pred[..., 7:9])\n",
    "    mse_L_foot = mse_loss(y_true[..., 13:15], y_pred[..., 13:15])\n",
    "\n",
    "    # Add the feature-specific losses to the overall MSE\n",
    "    mse = mse + mse_R_foot + mse_L_foot\n",
    "\n",
    "    # Add the prediction restriction loss and top-24 MSE to the total loss\n",
    "    mse_1 = pred_restric_loss(y_true, y_pred)\n",
    "    mse_2 = mse_loss_top_24(y_true, y_pred)\n",
    "    mse = 2 * mse + 4 * mse_1 + mse_2\n",
    "\n",
    "    # Take the square root of the final loss to scale it down\n",
    "    mse = tf.sqrt(mse)\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105308,
     "status": "ok",
     "timestamp": 1735374812767,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "C8o-SvgmZDwS",
    "outputId": "d4f87f0e-03d5-4a3e-9c20-da715f7f43ac"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # Import TensorFlow for model loading and predictions\n",
    "\n",
    "# Static files initialization\n",
    "parti_data = {}  # Dictionary to store data for each participant\n",
    "participants = ['pp054']  # List of participants (can be extended to include more)\n",
    "Sub = ['S01', 'S02', 'S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18']  # List of subjects\n",
    "\n",
    "# Loop through each participant\n",
    "for parti in participants:\n",
    "    parti_data[parti] = {}  # Initialize dictionary for the current participant's data\n",
    "\n",
    "    # Loop through each subject\n",
    "    for s in Sub:\n",
    "        Select_subject = parti  # Set the selected subject (current participant in this case)\n",
    "\n",
    "        # Call the train_test function to get the test data and labels for the selected subject\n",
    "        x_test_f, subject_test_f = train_test(data_LASI, train, Select_subject)\n",
    "\n",
    "        # Check if there is a GPU available for TensorFlow (optional)\n",
    "        device = tf.test.gpu_device_name()\n",
    "\n",
    "        # Load the pre-trained model specific to the current subject\n",
    "        model_2 = tf.keras.models.load_model(f\"./models//{s}_30_11_2024_walk_model.keras\", compile=False)\n",
    "\n",
    "        # Predict using the model, feeding in the test data (note: slicing the input as required)\n",
    "        pred = model_2.predict(x_test_f[:, :, :])\n",
    "\n",
    "        data = {}  # Initialize an empty dictionary to store marker data\n",
    "        markers = ['LASI', 'RASI', 'LPSI', 'RPSI', 'LTHI', 'LKNE', 'LTIB', 'LANK', 'LHEE', 'LTOE', 'RTHI', 'RKNE', 'RTIB', 'RANK', 'RHEE', 'RTOE']  # List of marker names\n",
    "\n",
    "        # Loop over each marker and store the predicted data\n",
    "        j = 0  # Initialize index for accessing different marker predictions\n",
    "        for i in markers:\n",
    "            # Store the predicted values (scaled by 10 for some units) for each marker\n",
    "            data[i] = {'y_pred': pred[:, :, :, j] * 10}\n",
    "            j += 1  # Increment the marker index\n",
    "\n",
    "        # Apply a low-pass filter to the predicted data for each marker\n",
    "        for i in markers:\n",
    "            for j in range(3):  # Loop through the 3 dimensions (e.g., x, y, z)\n",
    "                data[i]['y_pred'][:, :, j] = butter_lowpass_filter(data[i]['y_pred'][:, :, j])  # Apply filter\n",
    "\n",
    "        # Store the processed data for the current subject and participant\n",
    "        parti_data[parti][s] = data\n",
    "\n",
    "        # Save the processed data as a pickle file for future use\n",
    "        pd.to_pickle(data, f'{fld}//' + Select_subject + 'marker_prediction_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1735374855799,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "LCjY5-JDBPYr"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the mean data for each participant\n",
    "mean_data = {}\n",
    "\n",
    "# Loop through each participant in the 'participants' list\n",
    "for parti in participants:\n",
    "    mean_data[parti] = {}  # Create a sub-dictionary for the current participant\n",
    "\n",
    "    # Initialize the structure of 'mean_data' for each marker and condition in the first subject (S01)\n",
    "    for marker in parti_data[parti]['S01'].keys():\n",
    "        for condi in parti_data[parti]['S01'][marker].keys():\n",
    "            mean_data[parti][marker] = {}  # Create a dictionary for the marker\n",
    "            mean_data[parti][marker][condi] = {}  # Create a dictionary for the condition\n",
    "\n",
    "    # Initialize a variable 'i' to control the loop for accumulating data\n",
    "    i = 1\n",
    "\n",
    "    # Loop through each subject in the current participant's data\n",
    "    for sub in parti_data[parti].keys():\n",
    "\n",
    "        # Loop through each marker in the current subject's data\n",
    "        for marker in parti_data[parti][sub].keys():\n",
    "\n",
    "            # Loop through each condition in the current marker's data\n",
    "            for condi in parti_data[parti][sub][marker].keys():\n",
    "\n",
    "                # Fetch the data for the current condition (though it's not used directly here)\n",
    "                parti_data[parti][sub][marker][condi]\n",
    "\n",
    "                # If it's the first iteration (i==1), initialize the mean data for the condition\n",
    "                if i == 1:\n",
    "                    mean_data[parti][marker][condi] = parti_data[parti][sub][marker][condi]\n",
    "                else:\n",
    "                    # Otherwise, add the current data to the accumulated data\n",
    "                    mean_data[parti][marker][condi] = mean_data[parti][marker][condi] + parti_data[parti][sub][marker][condi]\n",
    "\n",
    "        # After processing the first subject, set i to 2 to accumulate data from subsequent subjects\n",
    "        i = 2\n",
    "\n",
    "\n",
    "# Loop through each participant in the 'participants' list\n",
    "for parti in participants:\n",
    "\n",
    "    # Loop through each marker in the current participant's mean data\n",
    "    for marker in mean_data[parti].keys():\n",
    "\n",
    "        # Loop through each condition for the current marker in the participant's data\n",
    "        for condi in parti_data[parti][sub][marker].keys():\n",
    "\n",
    "            # Normalize the accumulated data for each condition by dividing by 18\n",
    "            # This assumes that there are 18 subjects (or data points) contributing to the total sum\n",
    "            mean_data[parti][marker][condi] = mean_data[parti][marker][condi] / 18\n",
    "\n",
    "import pandas as pd  # Import pandas for data manipulation and saving in pickle format\n",
    "import pickle  # Import pickle for serializing Python objects (though it's not used explicitly here)\n",
    "from scipy.io import savemat  # Import savemat from scipy for saving data in .mat (MATLAB) format\n",
    "\n",
    "# Loop through each participant in the 'participants' list\n",
    "for parti in participants:\n",
    "\n",
    "    # Save the processed mean data for the current participant as a .pkl (pickle) file\n",
    "    # This serializes the data into a binary format which can later be loaded using pd.read_pickle\n",
    "    pd.to_pickle(mean_data[parti], f'{fld}\\\\Prediction\\\\{parti}_walk.pkl')\n",
    "\n",
    "    # Save the processed mean data as a .mat (MATLAB) file\n",
    "    # The 'savemat' function saves the data in a format that can be loaded in MATLAB\n",
    "    # The data is saved under the variable name 'data' inside the .mat file\n",
    "    savemat(f'{fld}\\\\Prediction\\\\{parti}_walk.mat', {'data': mean_data[parti]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sge1WZZvBPlA"
   },
   "outputs": [],
   "source": [
    "fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2335,
     "status": "ok",
     "timestamp": 1735369603276,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "l-Ngj9mKFoKO"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kineticstoolkit.lab as ktk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "error",
     "timestamp": 1735370184045,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "kJqfARscFyTp",
    "outputId": "22681455-7291-469c-bb11-e77961571b4c"
   },
   "outputs": [],
   "source": [
    "# Set an interactive backend\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1735374869862,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "SJRe_F9pGP5A"
   },
   "outputs": [],
   "source": [
    "fld='./data/pp054/imu/Prediction'\n",
    "fl=lab.find_files(path=fld,ext='pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1735374870684,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "nEb4DlkUFwf_"
   },
   "outputs": [],
   "source": [
    "# prompt: Filter data using 4th order Butterworth filter with 6hz cut off frequency and 100hz sampling rate\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=2):\n",
    "  nyq = 0.5 * fs\n",
    "  normal_cutoff = cutoff / nyq\n",
    "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "  y = filtfilt(b, a, data)\n",
    "  return y\n",
    "\n",
    "# Assuming 'data' is your time series data\n",
    "#filtered_data = butter_lowpass_filter(data, 6, 100, order=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1735375000817,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "Bzx1W2SZF-aY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_pickle(fl[0])\n",
    "\n",
    "import kineticstoolkit as ktk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Get a mask to identify the time indices within the desired range\n",
    "# Trim the time and all channels using the mask\n",
    "ts_trimmed = ktk.TimeSeries()\n",
    "time=np.array(range(0,101))/100\n",
    "ts_trimmed.time=time.reshape(101)\n",
    "for key1 in data.keys():\n",
    "        for key2 in data[key1].keys():\n",
    "            ndata=np.ones([101,4])\n",
    "            ndata[:,0:3]=data[key1][key2][1,:,:]/100\n",
    "            #if key2=='y_pred':\n",
    "                #ndata[:,0:3]=ndata[:,0:3]+[-0.0065,-0.12,0.04]\n",
    "            ndata[:,0]=butter_lowpass_filter(ndata[:,0], 6, 100, order=4)\n",
    "            ndata[:,1]=butter_lowpass_filter(ndata[:,1], 6, 100, order=4)\n",
    "            ndata[:,2]=butter_lowpass_filter(ndata[:,2], 6, 100, order=4)\n",
    "\n",
    "            ch_name=key2+':'+key1\n",
    "            ts_trimmed = ts_trimmed.add_data(ch_name,ndata)  # Trim all channels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1735375003306,
     "user": {
      "displayName": "Vaibhav Shah",
      "userId": "05555234280990842588"
     },
     "user_tz": 300
    },
    "id": "W-PocmTFGhDZ",
    "outputId": "f21d1aa9-a72d-4a24-972a-ec4ff03912a9"
   },
   "outputs": [],
   "source": [
    "import kineticstoolkit.lab as ktk\n",
    "\n",
    "# Download and read markers from a sample C3D file\n",
    "\n",
    "interconnections = dict()  # Will contain all segment definitions\n",
    "interconnections[\"LLowerLimb\"] = {\n",
    "    \"Color\": (1, 0, 0),  # In RGB format (here, greenish blue)\n",
    "    \"Links\": [  # List of lines that span lists of markers\n",
    "        [\"*LTOE\", \"*LHEE\", \"*LANK\", \"*LTOE\"],\n",
    "        [\"*LANK\", \"*LKNE\", \"*LASI\"],\n",
    "        [\"*LKNE\", \"*LPSI\"],\n",
    "    ],\n",
    "}\n",
    "interconnections[\"RLowerLimb\"] = {\n",
    "    \"Color\": (0, 1, 0),\n",
    "    \"Links\": [\n",
    "        [\"*RTOE\", \"*RHEE\", \"*RANK\", \"*RTOE\"],\n",
    "        [\"*RANK\", \"*RKNE\", \"*RASI\"],\n",
    "        [\"*RKNE\", \"*RPSI\"],\n",
    "    ],\n",
    "}\n",
    "interconnections[\"TrunkPelvis\"] = {\n",
    "    \"Color\": (0, 0, 1),\n",
    "    \"Links\": [\n",
    "        [\"*LPSI\", \"*LASI\", \"*RASI\", \"*RPSI\", \"*LPSI\"]\n",
    "\n",
    "    ],\n",
    "}\n",
    "p = ktk.Player(\n",
    "    ts_trimmed,\n",
    "    interconnections=interconnections,\n",
    "    up=\"z\",\n",
    "    anterior=\"-y\",\n",
    "    target=(0, 0.5, 1),\n",
    "    azimuth=0.1,\n",
    "    zoom=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjLUSMdLahKo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1l2mVxX7acCzl2x8_ssLTLuObg6DKV1Kx",
     "timestamp": 1735361360170
    }
   ]
  },
  "kernelspec": {
   "display_name": "Marker-position",
   "language": "python",
   "name": "marker-position"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
